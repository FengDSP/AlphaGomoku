{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gomoku game\n",
    "\n",
    "1. [DONE] Implement the Gomoku game rule, such as checking whether a position is the end of the game.\n",
    "2. [DONE] Implement a heuristic rule-based policy.\n",
    "3. [DONE] Implement an interactive playing UI to verify whether a policy works.\n",
    "4. [DONE] Use supervised learning to learn the rule-based policy at move level.\n",
    "5. [DONE] Implement a head-to-head evaluation to compare the performance of the policies.\n",
    "6. [DONE] Use reinforcement learning to learn a policy against itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Callable, Any\n",
    "from tqdm.notebook import tqdm\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IN_ROW_TO_WIN = 5\n",
    "BOARD_LENGTH = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111],\n",
       "        [100,  65,  66,  67,  68,  69,  70,  71,  72,  73, 112],\n",
       "        [ 99,  64,  37,  38,  39,  40,  41,  42,  43,  74, 113],\n",
       "        [ 98,  63,  36,  17,  18,  19,  20,  21,  44,  75, 114],\n",
       "        [ 97,  62,  35,  16,   5,   6,   7,  22,  45,  76, 115],\n",
       "        [ 96,  61,  34,  15,   4,   1,   8,  23,  46,  77, 116],\n",
       "        [ 95,  60,  33,  14,   3,   2,   9,  24,  47,  78, 117],\n",
       "        [ 94,  59,  32,  13,  12,  11,  10,  25,  48,  79, 118],\n",
       "        [ 93,  58,  31,  30,  29,  28,  27,  26,  49,  80, 119],\n",
       "        [ 92,  57,  56,  55,  54,  53,  52,  51,  50,  81, 120],\n",
       "        [ 91,  90,  89,  88,  87,  86,  85,  84,  83,  82, 121]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spiral_order(length) -> torch.Tensor:\n",
    "  assert length % 2 == 1, \"spiral_order only works for odd length\"\n",
    "  spiral_order = torch.zeros([length, length], dtype=torch.int64)\n",
    "  x, y = length // 2, length // 2\n",
    "  dx, dy = 0, 1\n",
    "  try:\n",
    "    for i in range(length * length):\n",
    "      spiral_order[x, y] = i + 1\n",
    "      if spiral_order[x + dy, y - dx] == 0:  # x + dx >= length or x + dx < 0 or y + dy >= length or y + dy < 0 or \n",
    "        dx, dy = dy, -dx\n",
    "      x, y = x + dx, y + dy\n",
    "  except IndexError:\n",
    "    print(f\"IndexError at {x}, {y}, i={i}, spiral_order={spiral_order}\")\n",
    "  return spiral_order\n",
    "\n",
    "spiral_order(BOARD_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test passed!\n"
     ]
    }
   ],
   "source": [
    "def is_win(\n",
    "    position: torch.LongTensor,\n",
    "    point: torch.LongTensor,\n",
    "  ) -> bool:\n",
    "  '''Check whether the position is at a terminating status.\n",
    "  Args:\n",
    "    position: A Tensor of shape [B, BOARD_LENGTH, BOARD_LENGTH] which has value 0, 1, and 2 for each element.\n",
    "      0 means the point is not ocupied and available to play on. 1 mean that the point is taken by player 1,\n",
    "      and 2 means that the point is taken by player 2.\n",
    "    point: A Tensor [B, 2], which carries the 0-based row and column of the point that is last played. The\n",
    "      position should be already updated with the last play, which means that position[point] should be either\n",
    "      1 or 2 and cannot be 0.\n",
    "  '''\n",
    "  \n",
    "  # check row\n",
    "  x_len = 1\n",
    "  for y_diff in range(1, NUM_IN_ROW_TO_WIN):\n",
    "    if point[1] + y_diff < BOARD_LENGTH and position[point[0], point[1] + y_diff] == position[point[0], point[1]]:\n",
    "      x_len += 1\n",
    "    else:\n",
    "      break\n",
    "  for y_diff in range(-1, -NUM_IN_ROW_TO_WIN, -1):\n",
    "    if point[1] + y_diff >= 0 and position[point[0], point[1] + y_diff] == position[point[0], point[1]]:\n",
    "      x_len += 1\n",
    "    else:\n",
    "      break\n",
    "  if x_len >= NUM_IN_ROW_TO_WIN:\n",
    "    return True\n",
    "  \n",
    "  # check column\n",
    "  y_len = 1\n",
    "  for x_diff in range(1, NUM_IN_ROW_TO_WIN):\n",
    "    if point[0] + x_diff < BOARD_LENGTH and position[point[0] + x_diff, point[1]] == position[point[0], point[1]]:\n",
    "      y_len += 1\n",
    "    else:\n",
    "      break\n",
    "  for x_diff in range(-1, -NUM_IN_ROW_TO_WIN, -1):\n",
    "    if point[0] + x_diff >= 0 and position[point[0] + x_diff, point[1]] == position[point[0], point[1]]:\n",
    "      y_len += 1\n",
    "    else:\n",
    "      break\n",
    "  if y_len >= NUM_IN_ROW_TO_WIN:\n",
    "    return True\n",
    "\n",
    "  # check top-right bottom-left diagonal\n",
    "  diag_len = 1\n",
    "  for diff in range(1, NUM_IN_ROW_TO_WIN):\n",
    "    if point[0] + diff < BOARD_LENGTH and point[1] + diff < BOARD_LENGTH and position[point[0] + diff, point[1] + diff] == position[point[0], point[1]]:\n",
    "      diag_len += 1\n",
    "    else:\n",
    "      break\n",
    "  for diff in range(-1, -NUM_IN_ROW_TO_WIN, -1):\n",
    "    if point[0] + diff >= 0 and point[1] + diff >= 0 and position[point[0] + diff, point[1] + diff] == position[point[0], point[1]]:\n",
    "      diag_len += 1\n",
    "    else:\n",
    "      break\n",
    "  if diag_len >= NUM_IN_ROW_TO_WIN:\n",
    "    return True\n",
    "\n",
    "  # check top-left bottom-right diagonal\n",
    "  diag_len = 1\n",
    "  for diff in range(1, NUM_IN_ROW_TO_WIN):\n",
    "    if point[0] + diff < BOARD_LENGTH and point[1] - diff >= 0 and position[point[0] + diff, point[1] - diff] == position[point[0], point[1]]:\n",
    "      diag_len += 1\n",
    "    else:\n",
    "      break\n",
    "  for diff in range(-1, -NUM_IN_ROW_TO_WIN, -1):\n",
    "    if point[0] + diff >= 0 and point[1] - diff < BOARD_LENGTH and position[point[0] + diff, point[1] - diff] == position[point[0], point[1]]:\n",
    "      diag_len += 1\n",
    "    else:\n",
    "      break\n",
    "  if diag_len >= NUM_IN_ROW_TO_WIN:\n",
    "    return True\n",
    "\n",
    "  return False\n",
    "\n",
    "\n",
    "def test_is_win():\n",
    "  position = torch.zeros([BOARD_LENGTH, BOARD_LENGTH], dtype=torch.int32)\n",
    "  position[0, 0] = 1\n",
    "  position[1, 0] = 1\n",
    "  position[2, 0] = 1\n",
    "  position[3, 0] = 1\n",
    "  position[4, 0] = 1\n",
    "  assert is_win(position, torch.tensor([4, 0]))\n",
    "  position[4, 0] = 0\n",
    "  position[0, 0] = 0\n",
    "  position[0, 1] = 1\n",
    "  position[0, 2] = 1\n",
    "  position[0, 3] = 1\n",
    "  position[0, 4] = 1\n",
    "  position[0, 5] = 1\n",
    "  assert is_win(position, torch.tensor([0, 3]))\n",
    "  position[0, 0] = 1\n",
    "  position[1, 1] = 1\n",
    "  position[2, 2] = 1\n",
    "  position[3, 3] = 1\n",
    "  position[4, 4] = 1\n",
    "  assert is_win(position, torch.tensor([1, 1]))\n",
    "  position[4, 4] = 0\n",
    "  position[5, 4] = 2\n",
    "  position[6, 3] = 2\n",
    "  position[7, 2] = 2\n",
    "  position[8, 1] = 2\n",
    "  position[9, 0] = 2\n",
    "  assert is_win(position, torch.tensor([9, 0]))\n",
    "  assert not is_win(position, torch.tensor([3, 0]))\n",
    "  print('All test passed!')\n",
    "\n",
    "\n",
    "test_is_win()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_filter() -> torch.LongTensor:  \n",
    "  row_filter = torch.zeros([NUM_IN_ROW_TO_WIN, NUM_IN_ROW_TO_WIN], dtype=torch.long)\n",
    "  row_filter[NUM_IN_ROW_TO_WIN // 2] = 1\n",
    "  column_filter = torch.zeros([NUM_IN_ROW_TO_WIN, NUM_IN_ROW_TO_WIN], dtype=torch.long)\n",
    "  column_filter[:, NUM_IN_ROW_TO_WIN // 2] = 1\n",
    "  diag_filter = torch.zeros([NUM_IN_ROW_TO_WIN, NUM_IN_ROW_TO_WIN], dtype=torch.long)\n",
    "  diag_filter[range(NUM_IN_ROW_TO_WIN), range(NUM_IN_ROW_TO_WIN)] = 1\n",
    "  anti_diag_filter = torch.zeros([NUM_IN_ROW_TO_WIN, NUM_IN_ROW_TO_WIN], dtype=torch.long)\n",
    "  anti_diag_filter[range(NUM_IN_ROW_TO_WIN), range(NUM_IN_ROW_TO_WIN)[::-1]] = 1\n",
    "  return torch.stack([row_filter, column_filter, diag_filter, anti_diag_filter]).unsqueeze(1)\n",
    "\n",
    "_is_win_filter = win_filter()\n",
    "\n",
    "def is_player_win(position: torch.LongTensor, player: int) -> bool:\n",
    "  p = (position == player).long().unsqueeze(0).unsqueeze(0)\n",
    "  conved = F.conv2d(p, _is_win_filter, padding=NUM_IN_ROW_TO_WIN // 2)\n",
    "  return torch.any(conved == NUM_IN_ROW_TO_WIN).detach().item()\n",
    "\n",
    "\n",
    "def test_is_player_win():\n",
    "  position = torch.zeros([BOARD_LENGTH, BOARD_LENGTH], dtype=torch.int32)\n",
    "  position[0, 0] = 1\n",
    "  position[1, 0] = 1\n",
    "  position[2, 0] = 1\n",
    "  position[3, 0] = 1\n",
    "  assert not is_player_win(position, 1)\n",
    "  position[4, 0] = 1\n",
    "  assert is_player_win(position, 1)\n",
    "  assert not is_player_win(position, 2)\n",
    "  position[4, 0] = 0\n",
    "  position[0, 1] = 1\n",
    "  assert not is_player_win(position, 1)\n",
    "\n",
    "test_is_player_win()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test passed!\n"
     ]
    }
   ],
   "source": [
    "# use a spiral order to break ties\n",
    "break_tie = spiral_order(BOARD_LENGTH) * -0.00001\n",
    "\n",
    "def greedy_play(\n",
    "    position: torch.Tensor,\n",
    "    player: int,\n",
    ") -> torch.Tensor | None:\n",
    "  '''Given the current position, return the next play for the player.\n",
    "  Args:\n",
    "    position: A tensor of shape [BOARD_LENGTH, BOARD_LENGTH] which has value 0, 1, and 2 for each element.\n",
    "      0 means the point is not ocupied and available to play on. 1 mean that the point is taken by player 1,\n",
    "      and 2 means that the point is taken by player 2.\n",
    "    player: An integer, 1 or 2, which indicates the player to play next.\n",
    "  Returns:\n",
    "    A tensor of shape [2], which carries the 0-based row and column of the point that the player should play.\n",
    "    If the player cannot play anywhere, return None.\n",
    "  '''\n",
    "  # Score every empty point on the board:\n",
    "  # For the 5 consecutive points starting from the point on one of the 4 directions (right, right-down, down, left-down):\n",
    "  #   1. If the last point of the 5 is out of the board, skip.\n",
    "  #   2. If both players have taken at least one of the 5 points, skip.\n",
    "  #   3. If 4/3/2/1 point(s) are alreay taken by the player, score at the empty positions +100000/1000/10/1.\n",
    "  #   4. If 4/3/2/1 point(s) are alreay taken by the other player, score at the empty positions +10000/100/1/0.1.\n",
    "  # return the point with the highest score. If there is a tie, return the point closest to the center of the board.\n",
    "  # If there is a tie, return the one with the smallest row.\n",
    "  # If still tie, return the one with the smallest column.\n",
    "\n",
    "  scores = break_tie.clone()\n",
    "  for dx, dy in [(0, 1), (1, 1), (1, 0), (1, -1)]:\n",
    "    counts_x = BOARD_LENGTH - (NUM_IN_ROW_TO_WIN - 1) * abs(dx)\n",
    "    counts_y = BOARD_LENGTH - (NUM_IN_ROW_TO_WIN - 1) * abs(dy)\n",
    "    def _ith_on_direction(board, i):\n",
    "      x_begin = dx * i\n",
    "      y_begin = dy * i if dy >= 0 else dy * i + NUM_IN_ROW_TO_WIN - 1\n",
    "      return board[x_begin: x_begin + counts_x, y_begin: y_begin + counts_y]\n",
    "    \n",
    "    # First calculate scores for potential 5s taken by the player\n",
    "    count_begin_at = torch.zeros([counts_x, counts_y], dtype=torch.int64)\n",
    "    for i in range(NUM_IN_ROW_TO_WIN):\n",
    "      # if player, +1, if other player, -100, otherwise 0\n",
    "      position_view = _ith_on_direction(position, i)\n",
    "      count_begin_at += torch.where(position_view == player, 1, 0)\n",
    "      count_begin_at -= torch.where(position_view == 3 - player, 100, 0)\n",
    "    for i in range(NUM_IN_ROW_TO_WIN):\n",
    "      score_view = _ith_on_direction(scores, i)\n",
    "      # no need to pay attention to the points that are already taken here. We deal with them before the return.\n",
    "      score_view += torch.where(count_begin_at == 4, 100000, 0)\n",
    "      score_view += torch.where(count_begin_at == 3, 1000, 0)\n",
    "      score_view += torch.where(count_begin_at == 2, 10, 0)\n",
    "      score_view += torch.where(count_begin_at == 1, 1, 0)\n",
    "    \n",
    "    # # Then calculate scores for potential 5s taken by the other player\n",
    "    count_begin_at = torch.zeros([counts_x, counts_y], dtype=torch.int64)\n",
    "    for i in range(NUM_IN_ROW_TO_WIN):\n",
    "      position_view = _ith_on_direction(position, i)\n",
    "      count_begin_at += torch.where(position_view == 3 - player, 1, 0)\n",
    "      count_begin_at -= torch.where(position_view == player, 100, 0)\n",
    "    for i in range(NUM_IN_ROW_TO_WIN):\n",
    "      score_view = _ith_on_direction(scores, i)\n",
    "      score_view += torch.where(count_begin_at == 4, 10000, 0)\n",
    "      score_view += torch.where(count_begin_at == 3, 100, 0)\n",
    "      score_view += torch.where(count_begin_at == 2, 1, 0)\n",
    "      score_view += torch.where(count_begin_at == 1, 0.1, 0)\n",
    "\n",
    "  # only count the scores for empty positions\n",
    "  scores[position != 0] = -1000000\n",
    "  best_index = torch.argmax(scores, axis=None)\n",
    "  best_row = best_index // BOARD_LENGTH\n",
    "  best_col = best_index % BOARD_LENGTH\n",
    "  return torch.tensor([best_row, best_col])\n",
    "\n",
    "\n",
    "def test_greedy_play():\n",
    "  position = torch.zeros([BOARD_LENGTH, BOARD_LENGTH], dtype=torch.int32)\n",
    "  position[1, 0] = 1\n",
    "  position[2, 0] = 1\n",
    "  position[3, 0] = 1\n",
    "  \n",
    "  assert torch.all(greedy_play(position, 1) == torch.tensor([4, 0])), f\"greedy_play(position, 1)={greedy_play(position, 1)}\"\n",
    "  # TODO: add more tests\n",
    "  print('All test passed!')\n",
    "\n",
    "\n",
    "test_greedy_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractPlayer(ABC):\n",
    "  @abstractmethod\n",
    "  def play(self, position: torch.Tensor, current_player: int) -> torch.LongTensor:\n",
    "    raise NotImplementedError\n",
    "  \n",
    "\n",
    "class RuleBasedPlayer(ABC):\n",
    "  def play(self, position: torch.Tensor, current_player: int) -> torch.LongTensor:\n",
    "    return greedy_play(position, current_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cce3958981b4b0c9c1192e85384e23b",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=600.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI plays at 5, 5\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "class InteractivePlay(object):\n",
    "  def __init__(self, *, ai_player=None, ai_first=False):\n",
    "    self.ai_player = ai_player\n",
    "    self.position = torch.zeros([BOARD_LENGTH, BOARD_LENGTH], dtype=torch.int32)\n",
    "    self.ai_first = ai_first\n",
    "    self.fig = None\n",
    "    self.cid = None\n",
    "\n",
    "  def _play_for_ai(self) -> Tuple[int, int]:\n",
    "      ai_x, ai_y = self.ai_player(self.position, 2)\n",
    "      assert self.position[ai_x, ai_y] == 0, f\"AI player tried to play at {ai_x}, {ai_y} which is already taken.\"\n",
    "      self.position[ai_x, ai_y] = 2\n",
    "      print(f\"AI plays at {ai_x}, {ai_y}\")\n",
    "      plt.plot(ai_x + 0.5, ai_y + 0.5, 'go', markersize=20)\n",
    "      plt.draw()\n",
    "      return ai_x, ai_y\n",
    "\n",
    "  def update_board(self, event):\n",
    "    # TODO: Handle Draw.\n",
    "    x, y = int(event.xdata), int(event.ydata)\n",
    "    if x < 0 or x >= BOARD_LENGTH or y < 0 or y >= BOARD_LENGTH:\n",
    "        print(\"Out of board click. X=\", x, \"Y=\", y)\n",
    "        return\n",
    "    if self.position[x, y] != 0:  # If the clicked cell is empty\n",
    "        return\n",
    "    \n",
    "    self.position[x, y] = 1  # Place the player's piece\n",
    "    print(f\"Player plays at {x}, {y}\")\n",
    "    plt.plot(x + 0.5, y + 0.5, 'ro', markersize=20)\n",
    "    plt.draw()\n",
    "    # Check for win after updating the board\n",
    "    if is_win(self.position, (x, y)):\n",
    "        print(f\"Player wins!\")\n",
    "        self.fig.canvas.mpl_disconnect(self.cid)  # Disable further clicks\n",
    "        return\n",
    "\n",
    "    ai_x, ai_y = self._play_for_ai()\n",
    "    # Check for win after updating the board\n",
    "    if is_win(self.position, (ai_x, ai_y)):\n",
    "        print(f\"AI wins!\")\n",
    "        self.fig.canvas.mpl_disconnect(self.cid)  # Disable further clicks\n",
    "        return\n",
    "\n",
    "  def display_board(self):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xticks(torch.arange(BOARD_LENGTH) + 0.5, minor=True)\n",
    "    ax.set_yticks(torch.arange(BOARD_LENGTH) + 0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=2)\n",
    "    ax.tick_params(which=\"minor\", size=0)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(self.position, cmap='coolwarm', extent=[0, BOARD_LENGTH, 0, BOARD_LENGTH])\n",
    "    plt.gca().invert_yaxis()\n",
    "    self.fig = fig\n",
    "    self.cid = fig.canvas.mpl_connect('button_press_event', self.update_board)\n",
    "    plt.show()\n",
    "    if self.ai_first:\n",
    "       self._play_for_ai()\n",
    "\n",
    "\n",
    "play = InteractivePlay(ai_player=greedy_play, ai_first=True)\n",
    "play.display_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 8])\n",
      "(tensor([ 0.0565,  0.0790,  0.3006,  0.1553,  0.3102,  0.3436,  0.1762,  0.1556,\n",
      "         0.0446,  0.1758,  0.0299, -0.0668, -0.4540, -0.1120, -0.3029,  0.0256,\n",
      "         0.1400,  0.0032, -0.0588,  0.0712,  0.1524,  0.1037, -0.0980, -0.5739,\n",
      "        -0.2068, -0.6446, -0.3081, -0.2462, -0.4278, -0.4266, -0.2370,  0.1261,\n",
      "         0.1113, -0.0078, -0.5777,  0.0899, -0.4343, -0.1840, -0.2700, -0.2944,\n",
      "        -0.3709, -0.1072,  0.1910, -0.0863, -0.1181, -0.5935, -0.0409, -0.4437,\n",
      "        -0.4560, -0.5503, -0.7158, -0.7373, -0.4535, -0.1231, -0.1307, -0.1057,\n",
      "        -0.5360,  0.0241, -0.1943, -0.1852,  0.0064, -0.2797, -0.3044, -0.1889,\n",
      "        -0.0716,  0.0366, -0.3065, -0.5963, -0.3511, -0.6164, -0.2700,  0.0088,\n",
      "        -0.2302, -0.3371, -0.4319, -0.1532,  0.0076, -0.2083, -0.5052, -0.3987,\n",
      "        -0.6141, -0.2352, -0.1151, -0.4309, -0.4107, -0.5605, -0.2032, -0.0583,\n",
      "        -0.1369, -0.6800, -0.7035, -0.9544, -0.8605, -0.5235, -0.4770, -0.4570,\n",
      "        -0.4156, -0.2093, -0.1579, -0.3135, -0.5113, -0.4505, -0.6352, -0.7445,\n",
      "        -0.6596, -0.5814, -0.4800, -0.5620, -0.2797, -0.1515,  0.0372,  0.0413,\n",
      "         0.0061, -0.1724, -0.0058,  0.0769,  0.0988,  0.0617, -0.1624,  0.0234,\n",
      "         0.0657], grad_fn=<ReshapeAliasBackward0>), tensor([0.1806], grad_fn=<AddBackward0>))\n",
      "559804\n"
     ]
    }
   ],
   "source": [
    "class NNPolicy(nn.Module):\n",
    "  def __init__(self, *, one_hot=True, he_init=True):\n",
    "    super(NNPolicy, self).__init__()\n",
    "    self.one_hot = one_hot\n",
    "    conv1_in = 3 if one_hot else 1\n",
    "    self.conv1 = nn.Conv2d(conv1_in, 32, kernel_size=9, padding=4)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "    self.conv3 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
    "    self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1) \n",
    "    self.conv_out = nn.Conv2d(256, 1, kernel_size=1)\n",
    "    self.value_conv = nn.Conv2d(256, 1, kernel_size=1)\n",
    "    self.value_out = nn.Linear(BOARD_LENGTH * BOARD_LENGTH, 1)\n",
    "    if he_init:\n",
    "      torch.nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "      torch.nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "      torch.nn.init.kaiming_uniform_(self.conv3.weight, nonlinearity='relu')\n",
    "      torch.nn.init.kaiming_uniform_(self.conv4.weight, nonlinearity='relu')\n",
    "      torch.nn.init.kaiming_uniform_(self.conv_out.weight)\n",
    "      torch.nn.init.kaiming_uniform_(self.value_conv.weight)\n",
    "      torch.nn.init.kaiming_uniform_(self.value_out.weight)\n",
    "\n",
    "  def forward(self, x: torch.LongTensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x: A LongTensor of shape [B, L, L] with value 0, 1 or 2.\n",
    "\n",
    "    Return:\n",
    "      A Tensor of shape [B, L * L] for the logits to play at each position.\n",
    "    \"\"\"\n",
    "    assert x.dim() in [2, 3] and x.shape[-2:] == torch.Size([BOARD_LENGTH, BOARD_LENGTH]), f\"Unsupported input shape {x.shape}\"\n",
    "    output_mask = torch.where(x == 0, 0, -torch.inf)\n",
    "    if self.one_hot:\n",
    "      space_channel = torch.where(x == 0, 1.0, 0.0)\n",
    "      opponent_channel = torch.where(x == 1, 1.0, 0.0)\n",
    "      player_channel = torch.where(x == 2, 1.0, 0.0)\n",
    "      x = torch.stack([space_channel, opponent_channel, player_channel], dim=-3)\n",
    "    else:\n",
    "      x = x.float()\n",
    "      x = x + 1  # leave value 0 for paddings\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x = F.relu(self.conv4(x))\n",
    "    move_logits = self.conv_out(x).squeeze(-3) + output_mask\n",
    "    x = F.relu(self.value_conv(x)).squeeze(-3)\n",
    "    value = self.value_out(x.flatten(-2))\n",
    "    return move_logits.flatten(-2), value\n",
    "  \n",
    "\n",
    "class PolicyPlayer(object):\n",
    "  def __init__(self, policy: NNPolicy, greedy_play: bool=True):\n",
    "    self.policy = policy\n",
    "    self.greedy_play = greedy_play\n",
    "\n",
    "  def play(self, position: torch.Tensor, current_player: int) -> torch.Tensor:\n",
    "    self.policy.eval()\n",
    "    with torch.no_grad():\n",
    "      if (position != 0).all():\n",
    "        return None\n",
    "      assert current_player in [1, 2]\n",
    "      # Always play as player 2 for simplicity.\n",
    "      if current_player == 1:\n",
    "        position = torch.where(position > 0, 3 - position, 0)\n",
    "      logits, v = self.policy(position)\n",
    "      # softmax the last 2 dims of the output\n",
    "      action_probs = F.softmax(logits, dim=-1)\n",
    "      if self.greedy_play:\n",
    "        action_index = torch.argmax(action_probs)\n",
    "      else:\n",
    "        action_index = torch.multinomial(action_probs, 1)\n",
    "    return torch.LongTensor([action_index // BOARD_LENGTH, action_index % BOARD_LENGTH])\n",
    "\n",
    "\n",
    "# Test the PolicyPlayer and NNPolicy\n",
    "print(PolicyPlayer(NNPolicy(), greedy_play=False).play(torch.zeros([BOARD_LENGTH, BOARD_LENGTH], dtype=torch.int32), 1))\n",
    "print(NNPolicy()(torch.zeros([BOARD_LENGTH, BOARD_LENGTH])))\n",
    "print(sum(math.prod(p.shape) for p in NNPolicy().parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4398b544e1464fe6ac60404869657713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def scrape_player(\n",
    "    player: Callable[[torch.LongTensor, int], torch.LongTensor],\n",
    "    random_play=0,\n",
    ") -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    positions = []  # tensors of shape [BOARD_LENGTH, BOARD_LENGTH]\n",
    "    labels = []  # actions, tensors of (x, y) indices\n",
    "    wins = []  # episode tensors of value -1, 0, 1 for loss, draw, win\n",
    "    i = 0\n",
    "    progress = tqdm(total=BOARD_LENGTH * BOARD_LENGTH * (1 + random_play))\n",
    "    for i in range(BOARD_LENGTH):\n",
    "        for j in range(BOARD_LENGTH):\n",
    "            base_position = torch.zeros([BOARD_LENGTH, BOARD_LENGTH], dtype=torch.long)\n",
    "            base_position[i, j] = 1\n",
    "            current_player = 2\n",
    "            play_index = torch.LongTensor([i, j])\n",
    "            stone_num = 1\n",
    "            next_plays = [-1]\n",
    "            if random_play:\n",
    "                random_plays = np.random.choice(\n",
    "                    BOARD_LENGTH * BOARD_LENGTH, random_play, replace=False\n",
    "                )\n",
    "                next_plays.extend(random_plays)\n",
    "            for next_play in next_plays:\n",
    "                position = base_position.clone()\n",
    "                if next_play >= 0:\n",
    "                    row = next_play // BOARD_LENGTH\n",
    "                    column = next_play % BOARD_LENGTH\n",
    "                    position[row, column] = 2\n",
    "                    current_player = 1\n",
    "                    play_index = torch.LongTensor([row, column])\n",
    "                    stone_num = 2\n",
    "                episode_players = []\n",
    "                win_player = 0\n",
    "                while stone_num < BOARD_LENGTH * BOARD_LENGTH and win_player == 0:\n",
    "                    play_index = player(position, current_player)\n",
    "                    stone_num += 1\n",
    "                    # Always play as player 2 for simplicity.\n",
    "                    if current_player == 1:\n",
    "                        position_input = torch.where(position > 0, 3 - position, 0)\n",
    "                    else:\n",
    "                        position_input = position.clone()\n",
    "                    positions.append(position_input)\n",
    "                    labels.append(play_index.clone())\n",
    "                    episode_players.append(current_player)\n",
    "                    position[play_index[0], play_index[1]] = current_player\n",
    "                    if is_win(position, play_index):\n",
    "                        win_player = current_player\n",
    "                    current_player = 3 - current_player\n",
    "                if win_player == 0:  # draw game\n",
    "                    episode_win = torch.zeros(\n",
    "                        size=[len(episode_players)], dtype=torch.long\n",
    "                    )\n",
    "                else:\n",
    "                    episode_win = torch.where(\n",
    "                        torch.tensor(episode_players, dtype=torch.long) == win_player,\n",
    "                        1,\n",
    "                        -1,\n",
    "                    ).to(torch.long)\n",
    "                wins.append(episode_win)\n",
    "                progress.update()\n",
    "    wins_tensor = torch.concat(wins)\n",
    "    assert wins_tensor.shape[0] == len(positions)\n",
    "    return torch.stack(positions), torch.stack(labels), wins_tensor\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  scraped_positions, scraped_moves, scraped_results = scrape_player(greedy_play, random_play=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441178"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(scraped_positions, \"gomoku_scraped_positions.pth\")\n",
    "# torch.save(scraped_moves, \"gomoku_scraped_moves.pth\")\n",
    "# torch.save(scraped_results, \"gomoku_scraped_results.pth\")\n",
    "scraped_positions = torch.load(\"gomoku_scraped_positions.pth\")\n",
    "scraped_moves = torch.load(\"gomoku_scraped_moves.pth\")\n",
    "scraped_results = torch.load(\"gomoku_scraped_results.pth\")\n",
    "len(scraped_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 step 0: action_loss=4.450185298919678 value_loss=1.0107228755950928\n",
      "Epoch 0 step 1: action_loss=4.444716930389404 value_loss=0.9080188274383545\n",
      "Epoch 0 step 2: action_loss=4.363085746765137 value_loss=0.8061984777450562\n",
      "Epoch 0 step 3: action_loss=4.307687282562256 value_loss=0.7849788665771484\n",
      "Epoch 0 step 4: action_loss=4.2507805824279785 value_loss=0.7767808437347412\n",
      "Epoch 0 step 5: action_loss=4.159677505493164 value_loss=0.7981355786323547\n",
      "Epoch 0 step 6: action_loss=4.1376519203186035 value_loss=0.7899563908576965\n",
      "Epoch 0 step 7: action_loss=4.060267448425293 value_loss=0.779101550579071\n",
      "Epoch 0 step 8: action_loss=4.034036636352539 value_loss=0.8251941204071045\n",
      "Epoch 0 step 9: action_loss=3.9421069622039795 value_loss=0.8020758032798767\n",
      "Epoch 0 step 10: action_loss=3.929428815841675 value_loss=0.7942673563957214\n",
      "Epoch 0 step 11: action_loss=3.855156898498535 value_loss=0.7834046483039856\n",
      "Epoch 0 step 12: action_loss=3.7524852752685547 value_loss=0.7822356820106506\n",
      "Epoch 0 step 13: action_loss=3.820671796798706 value_loss=0.7952048182487488\n",
      "Epoch 0 step 14: action_loss=3.7793235778808594 value_loss=0.7991780042648315\n",
      "Epoch 0 step 15: action_loss=3.7986721992492676 value_loss=0.8000783324241638\n",
      "Epoch 0 step 16: action_loss=3.6443450450897217 value_loss=0.8141546249389648\n",
      "Epoch 0 step 17: action_loss=3.7815968990325928 value_loss=0.8104418516159058\n",
      "Epoch 0 step 18: action_loss=3.6935057640075684 value_loss=0.7691908478736877\n",
      "Epoch 0 step 19: action_loss=3.626638174057007 value_loss=0.8205112218856812\n",
      "Epoch 0 step 20: action_loss=3.5897557735443115 value_loss=0.8034935593605042\n",
      "Epoch 0 step 21: action_loss=3.6143131256103516 value_loss=0.79635089635849\n",
      "Epoch 0 step 22: action_loss=3.554264545440674 value_loss=0.7858384251594543\n",
      "Epoch 0 step 23: action_loss=3.4530577659606934 value_loss=0.7855055928230286\n",
      "Epoch 0 step 24: action_loss=3.498976469039917 value_loss=0.7984086871147156\n",
      "Epoch 0 step 25: action_loss=3.362011432647705 value_loss=0.7904229760169983\n",
      "Epoch 0 step 26: action_loss=3.4043710231781006 value_loss=0.8197346925735474\n",
      "Epoch 0 step 27: action_loss=3.455432176589966 value_loss=0.8083165287971497\n",
      "Epoch 0 step 28: action_loss=3.358398675918579 value_loss=0.7857051491737366\n",
      "Epoch 0 step 29: action_loss=3.2985596656799316 value_loss=0.7764832973480225\n",
      "Epoch 0 step 30: action_loss=3.2769668102264404 value_loss=0.799528956413269\n",
      "Epoch 0 step 31: action_loss=3.2596325874328613 value_loss=0.8111336827278137\n",
      "Epoch 0 step 32: action_loss=3.1882362365722656 value_loss=0.763232946395874\n",
      "Epoch 0 step 33: action_loss=3.130485773086548 value_loss=0.7717742919921875\n",
      "Epoch 0 step 34: action_loss=3.176097869873047 value_loss=0.8020901679992676\n",
      "Epoch 0 step 35: action_loss=3.101039409637451 value_loss=0.8027506470680237\n",
      "Epoch 0 step 36: action_loss=3.0177128314971924 value_loss=0.7637566924095154\n",
      "Epoch 0 step 37: action_loss=3.0347506999969482 value_loss=0.7764430046081543\n",
      "Epoch 0 step 38: action_loss=3.1142306327819824 value_loss=0.7891354560852051\n",
      "Epoch 0 step 39: action_loss=3.115907669067383 value_loss=0.7787112593650818\n",
      "Epoch 0 step 40: action_loss=3.0110390186309814 value_loss=0.7815534472465515\n",
      "Epoch 0 step 41: action_loss=3.0978429317474365 value_loss=0.802768349647522\n",
      "Epoch 0 step 42: action_loss=3.0151169300079346 value_loss=0.7600365877151489\n",
      "Epoch 0 step 43: action_loss=3.075728178024292 value_loss=0.8241946697235107\n",
      "Epoch 0 step 44: action_loss=2.867326498031616 value_loss=0.7812101244926453\n",
      "Epoch 0 step 45: action_loss=2.8777527809143066 value_loss=0.7343164086341858\n",
      "Epoch 0 step 46: action_loss=2.8225417137145996 value_loss=0.7985754013061523\n",
      "Epoch 0 step 47: action_loss=2.9645676612854004 value_loss=0.7875639796257019\n",
      "Epoch 0 step 48: action_loss=2.805959463119507 value_loss=0.7693707942962646\n",
      "Epoch 0 step 49: action_loss=2.677513360977173 value_loss=0.8028236031532288\n",
      "Epoch 0 step 100: action_loss=1.8373231887817383 value_loss=0.7760199904441833\n",
      "Epoch 0 step 200: action_loss=1.058454155921936 value_loss=0.7784974575042725\n",
      "Epoch 0 step 300: action_loss=1.013296365737915 value_loss=0.7870146632194519\n",
      "Epoch 0 step 400: action_loss=0.9896313548088074 value_loss=0.8148587942123413\n",
      "Epoch 0 step 500: action_loss=0.785453200340271 value_loss=0.8167953491210938\n",
      "Epoch 0 step 600: action_loss=0.9338913559913635 value_loss=0.7920302152633667\n",
      "Epoch 0 step 700: action_loss=0.8000501394271851 value_loss=0.7983872294425964\n",
      "Epoch 0 step 800: action_loss=0.708443820476532 value_loss=0.8260455131530762\n",
      "Epoch 0: test_action_loss=0.6938414573669434 test_value_loss=0.7900578379631042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c6/tvjpscjd4m96l0k_9kgcd2yh0000gn/T/ipykernel_7609/452352303.py:69: UserWarning: Using a target size (torch.Size([4411])) that is different to the input size (torch.Size([4411, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  test_value_loss = F.mse_loss(test_v, test_rewards)\n"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "    policy: NNPolicy,\n",
    "    train_positions: torch.Tensor,\n",
    "    train_labels: torch.Tensor,\n",
    "    train_rewards: torch.Tensor,\n",
    "    test_split=0.01,\n",
    "    lr=0.01,\n",
    "    lr_warmup_steps=100,\n",
    "    epochs=1,\n",
    "    batch_size=512,\n",
    "    train_value_head=True,\n",
    "    print_loss_every_steps=100,\n",
    ") -> NNPolicy:\n",
    "    # train test split\n",
    "    assert len(train_positions) == len(train_labels)\n",
    "    # flatten the label indices from [row, column] to a single index\n",
    "    train_labels = train_labels[..., 0] * BOARD_LENGTH + train_labels[..., 1]\n",
    "\n",
    "    N = len(train_positions)\n",
    "    test_N = int(N * test_split)\n",
    "    train_N = N - test_N\n",
    "    perm = torch.randperm(N)\n",
    "    test_indices = perm[:test_N]\n",
    "    train_indices = perm[test_N:]\n",
    "    test_positions = train_positions[test_indices]\n",
    "    test_labels = train_labels[test_indices]\n",
    "    test_rewards = train_rewards[test_indices]\n",
    "    train_positions = train_positions[train_indices]\n",
    "    train_labels = train_labels[train_indices]\n",
    "    train_rewards = train_rewards[train_indices]\n",
    "\n",
    "    policy.train()\n",
    "    optimizer = torch.optim.AdamW(policy.parameters(), lr=lr)\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < lr_warmup_steps:\n",
    "            return float(current_step) / float(max(1, lr_warmup_steps))\n",
    "        return 1.0\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(train_N)\n",
    "        for batch_begin in range(0, train_N - batch_size, batch_size):\n",
    "            step = batch_begin // batch_size\n",
    "            batch_indices = perm[batch_begin : batch_begin + batch_size]\n",
    "            batch_input = train_positions[batch_indices]\n",
    "            batch_label = train_labels[batch_indices]\n",
    "            batch_rewards = train_rewards[batch_indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits, v = policy(batch_input)\n",
    "            logits = logits.view(-1, BOARD_LENGTH * BOARD_LENGTH)\n",
    "            action_loss = F.cross_entropy(logits, target=batch_label)\n",
    "            if train_value_head:\n",
    "                value_loss = F.mse_loss(v.squeeze(-1), batch_rewards.to(torch.float))\n",
    "                loss = value_loss + action_loss\n",
    "                if step < 50 or step % print_loss_every_steps == 0:\n",
    "                    print(f\"Epoch {epoch} step {step}: action_loss={action_loss} value_loss={value_loss}\")\n",
    "            else:\n",
    "                loss = action_loss\n",
    "                if step < 50 or step % print_loss_every_steps == 0:\n",
    "                    print(f\"Epoch {epoch} step {step}: loss={action_loss}\")\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        test_logits, test_v = policy(test_positions)\n",
    "        test_logits = test_logits.view(-1, BOARD_LENGTH * BOARD_LENGTH)\n",
    "        test_action_loss = F.cross_entropy(test_logits, target=test_labels)\n",
    "        test_value_loss = F.mse_loss(test_v, test_rewards)\n",
    "        print(f\"Epoch {epoch}: test_action_loss={test_action_loss} test_value_loss={test_value_loss}\")\n",
    "\n",
    "    return policy\n",
    "\n",
    "trained_policy = train(\n",
    "    NNPolicy(),\n",
    "    scraped_positions,\n",
    "    scraped_moves,\n",
    "    scraped_results,\n",
    "    lr_warmup_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(trained_policy.state_dict(), \"gomoku_trained.pth\")\n",
    "trained_policy = torch.load(\"gomoku_trained.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings from Supervised Learning\n",
    "1. One-hot embedding is very important. Using a single integer to represent the board state took 10,000 steps for the loss to go down to 1.0. Using one-hot embedding took only 200 steps.\n",
    "2. The model performed poorly in the later game as observed from the interactive plays. It started to ignore the 4-in-a-rows from the player's play. In the beginning, the model played well by following the greedy rule. It's not clear why the model performed poorly in the later game. It might be because the training data has a high coverage of the early games because all early games are short so they are similar. The later games are more diverse so the model might not have enough training data to learn. Another reason might be that the padding doesn't work well, and later game often happens near the border.\n",
    "3. The model doesn't know where to put the first stone when it moves first. This is because the empty board state is not in the training data. We intentionally left the empty board as OOD, so that we can check if RL can get this fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 goes first: player1_wins=84 player2_wins=12 draw=4\n",
      "Player 2 goes first: player1_wins=45 player2_wins=49 draw=6\n",
      "Total: player1_wins=129 player2_wins=61 draw=10\n",
      " init_win=84.00% follow_win=45.00% init_draw=4.00% follow_draw=6.00% total_win=64.50% total_lose=30.50%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.645"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def head2head_ordered_eval(\n",
    "    player1: AbstractPlayer,\n",
    "    player2: AbstractPlayer,\n",
    "    num_plays: int,\n",
    ") -> Tuple[int, int, int]:\n",
    "    win1, win2, draw = 0, 0, 0\n",
    "    players = [player1, player2]\n",
    "\n",
    "    for _ in range(num_plays):\n",
    "        position = torch.zeros([BOARD_LENGTH, BOARD_LENGTH], dtype=torch.long)\n",
    "        position[BOARD_LENGTH // 2, BOARD_LENGTH // 2] = 1\n",
    "        current_player = 2\n",
    "        play_index = torch.LongTensor([BOARD_LENGTH // 2, BOARD_LENGTH // 2])\n",
    "        stone_num = 1\n",
    "        while stone_num < BOARD_LENGTH * BOARD_LENGTH:\n",
    "            if is_win(position, play_index):\n",
    "                if current_player == 1:\n",
    "                    win2 += 1\n",
    "                else:\n",
    "                    win1 += 1\n",
    "                break\n",
    "            play_index = players[current_player - 1].play(position, current_player)\n",
    "            stone_num += 1\n",
    "            position[play_index[0], play_index[1]] = current_player\n",
    "            current_player = 3 - current_player\n",
    "        if stone_num == BOARD_LENGTH * BOARD_LENGTH:\n",
    "            draw += 1\n",
    "    return win1, win2, draw\n",
    "\n",
    "\n",
    "def head2head_eval(\n",
    "    player1: AbstractPlayer,\n",
    "    player2: AbstractPlayer,\n",
    "    *,\n",
    "    num_plays: int = 100,\n",
    "    verbose_output: bool = True,\n",
    "    eval_name: str = \"\",\n",
    ") -> float:\n",
    "    p1f_w1, p1f_w2, p1f_draw = head2head_ordered_eval(\n",
    "        player1, player2, num_plays=num_plays\n",
    "    )\n",
    "    if verbose_output:\n",
    "        print(\n",
    "            f\"Player 1 goes first: player1_wins={p1f_w1} player2_wins={p1f_w2} draw={p1f_draw}\"\n",
    "        )\n",
    "    p2f_w2, p2f_w1, p2f_draw = head2head_ordered_eval(\n",
    "        player2, player1, num_plays=num_plays\n",
    "    )\n",
    "    if verbose_output:\n",
    "        print(\n",
    "            f\"Player 2 goes first: player1_wins={p2f_w1} player2_wins={p2f_w2} draw={p2f_draw}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Total: player1_wins={p1f_w1 + p2f_w1} player2_wins={p1f_w2 + p2f_w2} draw={p1f_draw + p2f_draw}\"\n",
    "        )\n",
    "    initiative_win = p1f_w1 / num_plays\n",
    "    init_draw = p1f_draw / num_plays\n",
    "    follow_up_win = p2f_w1 / num_plays\n",
    "    follow_draw = p2f_draw / num_plays\n",
    "    total_win = (p1f_w1 + p2f_w1) / (num_plays * 2)\n",
    "    total_lose = (p1f_w2 + p2f_w2) / (num_plays * 2)\n",
    "    print(\n",
    "        f\"{eval_name} init_win={initiative_win:.2%} follow_win={follow_up_win:.2%} init_draw={init_draw:.2%} follow_draw={follow_draw:.2%} total_win={total_win:.2%} total_lose={total_lose:.2%}\"\n",
    "    )\n",
    "    return total_win\n",
    "\n",
    "\n",
    "head2head_eval(\n",
    "    PolicyPlayer(trained_policy, greedy_play=True),\n",
    "    PolicyPlayer(trained_policy, greedy_play=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 goes first: player1_wins=10 player2_wins=0 draw=0\n",
      "Player 2 goes first: player1_wins=0 player2_wins=10 draw=0\n",
      "Total: player1_wins=10 player2_wins=10 draw=0\n",
      " init_win=100.00% follow_win=0.00% init_draw=0.00% follow_draw=0.00% total_win=50.00% total_lose=50.00%\n"
     ]
    }
   ],
   "source": [
    "_ = head2head_eval(\n",
    "    PolicyPlayer(trained_policy, greedy_play=True),\n",
    "    RuleBasedPlayer(),\n",
    "    num_plays=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 goes first: player1_wins=408 player2_wins=592 draw=0\n",
      "Player 2 goes first: player1_wins=39 player2_wins=961 draw=0\n",
      "Total: player1_wins=447 player2_wins=1553 draw=0\n",
      " init_win=40.80% follow_win=3.90% init_draw=0.00% follow_draw=0.00% total_win=22.35% total_lose=77.65%\n"
     ]
    }
   ],
   "source": [
    "_ = head2head_eval(\n",
    "    PolicyPlayer(trained_policy, greedy_play=False),\n",
    "    RuleBasedPlayer(),\n",
    "    num_plays=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Findings from Supervised Learning\n",
    "1. The supervised learning model is significantly worse than the rule based policy. While moving first the learned model only wins 20% of the time, while moving second it wins 0%.\n",
    "2. The greedy action selection happens to be not good. It loses to the rule based policy even when it moves first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_DRAW_REWARD = -0.01\n",
    "\n",
    "\n",
    "class GomokuGame:\n",
    "    \"\"\"Game rule implementation that is compatible with the MCTS below.\"\"\"\n",
    "\n",
    "    _signature_weights = torch.pow(\n",
    "        3, exponent=torch.arange(BOARD_LENGTH * BOARD_LENGTH)\n",
    "    )\n",
    "\n",
    "    def stringRepresentation(self, position: torch.LongTensor) -> int:\n",
    "        return (position.flatten() @ self._signature_weights).detach().item()\n",
    "\n",
    "    def getValidMoves(\n",
    "        self, position: torch.LongTensor, not_used_player: int\n",
    "    ) -> torch.LongTensor:\n",
    "        return position.flatten().detach().numpy() == 0\n",
    "    \n",
    "    def getActionSize(self,) -> int:\n",
    "        return BOARD_LENGTH * BOARD_LENGTH\n",
    "\n",
    "    def getNextState(\n",
    "        self, position: torch.LongTensor, current_player: int, action: int,\n",
    "    ) -> torch.LongTensor:\n",
    "        position = position.clone()\n",
    "        position[action // BOARD_LENGTH, action % BOARD_LENGTH] = current_player\n",
    "        return position, 3 - current_player\n",
    "    \n",
    "    def getGameEnded(\n",
    "        self,\n",
    "        position: torch.LongTensor,\n",
    "        player: int,  # 1 or 2\n",
    "    ) -> float:\n",
    "        if is_player_win(position, player):\n",
    "            return 1.0\n",
    "        elif is_player_win(position, 3 - player):\n",
    "            return -1.0\n",
    "        elif torch.all(position != 0):\n",
    "            return DEFAULT_DRAW_REWARD\n",
    "        else:\n",
    "            return 0.0  # not ended\n",
    "        \n",
    "    def getCanonicalForm(self, position, player):\n",
    "        # Always play as player 2 for simplicity.\n",
    "        if player == 1:\n",
    "            return torch.where(position != 0, 3 - position, 0)\n",
    "        else:\n",
    "            return position\n",
    "\n",
    "\n",
    "test_game = GomokuGame()\n",
    "test_position = torch.zeros([BOARD_LENGTH, BOARD_LENGTH], dtype=torch.long)\n",
    "test_position[:, 0] = 1\n",
    "test_position[:, 1] = 2\n",
    "test_game.getValidMoves(test_position, 0)\n",
    "test_position = torch.ones([BOARD_LENGTH, BOARD_LENGTH], dtype=torch.long)\n",
    "test_game.getGameEnded(test_position, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyWrap():\n",
    "    def __init__(self, nn: nn.Module):\n",
    "        self.nn = nn\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        with torch.no_grad():\n",
    "            t1, t2 = self.nn(*args, **kwds)\n",
    "            p = F.softmax(t1, dim=-1)\n",
    "            return p.numpy(), t2.item()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MCTSArgs():\n",
    "    numMCTSSims: int = 10\n",
    "    cpuct: float = 1.0\n",
    "\n",
    "# Below is copied from https://github.com/suragnair/alpha-zero-general/blob/master/MCTS.py\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}  # stores #times edge s,a was visited\n",
    "        self.Ns = {}  # stores #times board s was visited\n",
    "        self.Ps = {}  # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}  # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard, temp=1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for i in range(self.args.numMCTSSims):\n",
    "            self.search(canonicalBoard)\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "        if temp == 0:\n",
    "            bestAs = np.array(np.argwhere(counts == np.max(counts))).flatten()\n",
    "            bestA = np.random.choice(bestAs)\n",
    "            probs = [0] * len(counts)\n",
    "            probs[bestA] = 1\n",
    "            return probs\n",
    "\n",
    "        counts = [x ** (1. / temp) for x in counts]\n",
    "        counts_sum = float(sum(counts))\n",
    "        probs = [x / counts_sum for x in counts]\n",
    "        return probs\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # terminal node\n",
    "            return -self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # leaf node\n",
    "            self.Ps[s], v = self.nnet(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                print(f\"All valid moves were masked, doing a workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n",
    "                            1 + self.Nsa[(s, a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + EPS)  # Q = 0 ?\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            assert isinstance(self.Qsa[(s, a)], float), f\"type(self.Qsa[(s, a)])={type(self.Qsa[(s, a)])}\"\n",
    "            self.Nsa[(s, a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            assert isinstance(self.Qsa[(s, a)], float), f\"type(self.Qsa[(s, a)])={type(self.Qsa[(s, a)])}\"\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play(\n",
    "    policy: NNPolicy,\n",
    "    *,\n",
    "    policy_2: NNPolicy | None = None,\n",
    "    draw_reward: float,\n",
    "    split_reward: bool = False,\n",
    "    first_player: int = 1,\n",
    "    use_mcts: bool = False,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.LongTensor, torch.Tensor, int]:\n",
    "    batch_logits = []\n",
    "    batch_values = []\n",
    "    batch_plays = []\n",
    "    batch_players = []\n",
    "\n",
    "    position = torch.zeros(size=(BOARD_LENGTH, BOARD_LENGTH), dtype=torch.long)\n",
    "    current_player = first_player\n",
    "    num_stones = 0\n",
    "    win_player = 0\n",
    "    if use_mcts:\n",
    "        mcts = MCTS(GomokuGame(), NumpyWrap(policy), MCTSArgs())\n",
    "    while num_stones < BOARD_LENGTH * BOARD_LENGTH:\n",
    "        # Always play as player 2 for simplicity.\n",
    "        if current_player == 1:\n",
    "            position_tensor = torch.where(position > 0, 3 - position, 0).detach()\n",
    "        else:\n",
    "            position_tensor = position.detach()\n",
    "        _policy = policy if current_player == 1 or not policy_2 else policy_2\n",
    "        logits, position_v = _policy(position_tensor)\n",
    "        flattened_logits = logits.view(BOARD_LENGTH * BOARD_LENGTH)\n",
    "        if use_mcts:\n",
    "            action_probs = mcts.getActionProb(position_tensor)\n",
    "            to_play_flattend = np.random.choice(len(action_probs), p=action_probs)\n",
    "            to_play_flattend = torch.tensor(to_play_flattend, dtype=torch.long)\n",
    "        else:\n",
    "            flattened_probs = F.softmax(flattened_logits, dim=-1)\n",
    "            to_play_flattend = torch.multinomial(flattened_probs, 1)\n",
    "        if current_player == 1 or not policy_2:\n",
    "            batch_logits.append(flattened_logits)\n",
    "            batch_values.append(position_v)\n",
    "            batch_plays.append(to_play_flattend)\n",
    "            batch_players.append(current_player)\n",
    "\n",
    "        to_play_i = to_play_flattend.item() // BOARD_LENGTH\n",
    "        to_play_j = to_play_flattend.item() % BOARD_LENGTH\n",
    "        position[to_play_i, to_play_j] = current_player\n",
    "        num_stones += 1\n",
    "        if is_win(position, [to_play_i, to_play_j]):\n",
    "            win_player = current_player\n",
    "            break\n",
    "        current_player = 3 - current_player\n",
    "    _logits = torch.stack(batch_logits)\n",
    "    _values = torch.stack(batch_values)\n",
    "    _plays = torch.stack(batch_plays)\n",
    "    batch_players_tensor = torch.LongTensor(batch_players)\n",
    "    loss_player_stones = num_stones // 2\n",
    "    win_player_stones = num_stones - loss_player_stones\n",
    "    if win_player == 0:\n",
    "        _rewards = (\n",
    "            torch.ones_like(batch_players_tensor, dtype=torch.float) * draw_reward\n",
    "        )\n",
    "    elif split_reward:\n",
    "        _rewards = torch.where(\n",
    "            batch_players_tensor == win_player,\n",
    "            1.0 / win_player_stones,\n",
    "            -1.0 / loss_player_stones,\n",
    "        )\n",
    "    else:\n",
    "        _rewards = torch.where(\n",
    "            batch_players_tensor == win_player,\n",
    "            1.0,\n",
    "            -1.0,\n",
    "        )\n",
    "    return _logits, _values, _plays, _rewards, win_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforcement_learn(\n",
    "    base_policy: NNPolicy,\n",
    "    *,\n",
    "    lr: float = 0.001,\n",
    "    lr_schedule: str | None = None,\n",
    "    batch_size: int = 64,\n",
    "    steps: int = 16,\n",
    "    draw_reward: float = DEFAULT_DRAW_REWARD,\n",
    "    policy_as_player2: bool = True,\n",
    "    fixed_opponent_policy: NNPolicy | None = None,\n",
    "    opponent_update_win_rate: float = 0.55,\n",
    "    use_mcts: bool = False,\n",
    "    eval_every_steps: int = 1,\n",
    "    return_snapshots: bool = False,\n",
    ") -> nn.Module:\n",
    "    if use_mcts:\n",
    "        assert policy_as_player2, \"MCTS with a fixed based model is not implemented.\"  \n",
    "    policy = copy.deepcopy(base_policy)\n",
    "    if fixed_opponent_policy:\n",
    "        base_policy = fixed_opponent_policy\n",
    "    base_policy.eval()\n",
    "    optimizer = torch.optim.AdamW(policy.parameters(), lr=lr)\n",
    "    if not lr_schedule:\n",
    "        scheduler = None\n",
    "    elif lr_schedule == \"cosine\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=steps, eta_min=lr / 10\n",
    "        )\n",
    "    elif lr_schedule == \"exponential\":\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95, verbose=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Not supported lr_schedule: {lr_schedule}\")\n",
    "\n",
    "    base_policy_updates = 0\n",
    "    snapshots = []\n",
    "    for step in range(steps):\n",
    "        all_logits = []\n",
    "        all_values = []\n",
    "        all_plays = []\n",
    "        all_rewards = []\n",
    "        policy.train()\n",
    "        init_win_num = 0\n",
    "        follow_up_win_num = 0\n",
    "        draw_num = 0\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            if policy_as_player2:\n",
    "                batch_logits, batch_values, batch_plays, batch_rewards, win_player = self_play(\n",
    "                    policy, draw_reward=draw_reward, use_mcts=use_mcts,\n",
    "                )\n",
    "            else:\n",
    "                first_player = b % 2 + 1\n",
    "                batch_logits, batch_values, batch_plays, batch_rewards, win_player = self_play(\n",
    "                    policy, policy_2=base_policy, draw_reward=draw_reward, first_player=first_player,\n",
    "                )\n",
    "            all_logits.append(batch_logits)\n",
    "            all_values.append(batch_values)\n",
    "            all_plays.append(batch_plays)\n",
    "            all_rewards.append(batch_rewards)\n",
    "            if win_player == 0:\n",
    "                draw_num += 1\n",
    "            elif win_player == 1 and not policy_as_player2:\n",
    "                if first_player == 1:\n",
    "                    init_win_num += 1\n",
    "                else:\n",
    "                    follow_up_win_num += 1\n",
    "        step_logits = torch.concat(all_logits)  # N, BL*BL\n",
    "        step_values = torch.concat(all_values).squeeze(-1)\n",
    "        step_plays = torch.concat(all_plays).squeeze().detach()  # N\n",
    "        step_rewards = torch.concat(all_rewards).detach()  # N\n",
    "        step_loss = F.cross_entropy(step_logits, step_plays, reduction=\"none\")\n",
    "        rl_loss = (step_loss * step_rewards).mean()\n",
    "        if use_mcts:\n",
    "            value_loss = F.mse_loss(step_values, step_rewards)\n",
    "            final_loss = rl_loss + value_loss\n",
    "            value_loss_print = f\"value_loss={value_loss}\"\n",
    "        else:\n",
    "            final_loss = rl_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        if not policy_as_player2:\n",
    "            detailed_stats = f\"i{init_win_num}f{follow_up_win_num}d{draw_num}\"\n",
    "        else:\n",
    "            detailed_stats = f\"d{draw_num}\"\n",
    "        print(\n",
    "            f\"Step {step}: trained on {len(step_logits)} plays({detailed_stats}). loss={rl_loss} {value_loss_print if use_mcts else ''}\"\n",
    "        )\n",
    "        if not policy_as_player2 and not fixed_opponent_policy:\n",
    "            win_rate = head2head_eval(\n",
    "                PolicyPlayer(policy, greedy_play=False),\n",
    "                PolicyPlayer(base_policy, greedy_play=False),\n",
    "                verbose_output=False,\n",
    "                eval_name=\"Against base: \"\n",
    "            )\n",
    "            if win_rate > opponent_update_win_rate:\n",
    "                print(f\"<--Updating base policy.-->\")\n",
    "                base_policy = copy.deepcopy(policy)\n",
    "                base_policy.eval()\n",
    "                base_policy_updates += 1\n",
    "        if step % eval_every_steps == 0:\n",
    "            head2head_eval(\n",
    "                PolicyPlayer(policy, greedy_play=False),\n",
    "                RuleBasedPlayer(),\n",
    "                verbose_output=False,\n",
    "                eval_name=\"Against rule-based player: \"\n",
    "            )\n",
    "            snapshots.append(copy.deepcopy(policy))\n",
    "    print(f\"RL train finished! Policy update steps = {base_policy_updates}\")\n",
    "    return snapshots if return_snapshots else policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: trained on 1996 plays(d0). loss=-0.03280669450759888 \n",
      "Against rule-based player:  init_win=38.00% follow_win=4.00% init_draw=0.00% follow_draw=0.00% total_win=21.00% total_lose=79.00%\n",
      "Step 1: trained on 1787 plays(d0). loss=-0.019377373158931732 \n",
      "Against rule-based player:  init_win=49.00% follow_win=2.00% init_draw=0.00% follow_draw=0.00% total_win=25.50% total_lose=74.50%\n",
      "Step 2: trained on 1729 plays(d0). loss=-0.03688858076930046 \n",
      "Against rule-based player:  init_win=54.00% follow_win=0.00% init_draw=0.00% follow_draw=0.00% total_win=27.00% total_lose=73.00%\n",
      "Step 3: trained on 1661 plays(d0). loss=-0.03354712575674057 \n",
      "Against rule-based player:  init_win=66.00% follow_win=0.00% init_draw=0.00% follow_draw=0.00% total_win=33.00% total_lose=67.00%\n",
      "Step 4: trained on 1874 plays(d0). loss=-0.057748615741729736 \n",
      "Against rule-based player:  init_win=68.00% follow_win=0.00% init_draw=0.00% follow_draw=0.00% total_win=34.00% total_lose=66.00%\n",
      "Step 5: trained on 2087 plays(d0). loss=-0.016734469681978226 \n",
      "Against rule-based player:  init_win=69.00% follow_win=1.00% init_draw=0.00% follow_draw=0.00% total_win=35.00% total_lose=65.00%\n",
      "Step 6: trained on 2113 plays(d0). loss=-0.01609187386929989 \n",
      "Against rule-based player:  init_win=75.00% follow_win=0.00% init_draw=0.00% follow_draw=0.00% total_win=37.50% total_lose=62.50%\n",
      "Step 7: trained on 2287 plays(d0). loss=-0.020186547189950943 \n",
      "Against rule-based player:  init_win=73.00% follow_win=1.00% init_draw=0.00% follow_draw=0.00% total_win=37.00% total_lose=63.00%\n",
      "Step 8: trained on 2598 plays(d1). loss=0.008029356598854065 \n",
      "Against rule-based player:  init_win=80.00% follow_win=2.00% init_draw=0.00% follow_draw=0.00% total_win=41.00% total_lose=59.00%\n",
      "Step 9: trained on 2297 plays(d0). loss=-0.003333727363497019 \n",
      "Against rule-based player:  init_win=82.00% follow_win=0.00% init_draw=0.00% follow_draw=0.00% total_win=41.00% total_lose=59.00%\n",
      "Step 10: trained on 2868 plays(d3). loss=-0.035300783812999725 \n",
      "Against rule-based player:  init_win=80.00% follow_win=3.00% init_draw=0.00% follow_draw=0.00% total_win=41.50% total_lose=58.50%\n",
      "Step 11: trained on 2640 plays(d4). loss=0.0027565951459109783 \n",
      "Against rule-based player:  init_win=82.00% follow_win=0.00% init_draw=1.00% follow_draw=0.00% total_win=41.00% total_lose=58.50%\n",
      "Step 12: trained on 2698 plays(d4). loss=-0.024785950779914856 \n",
      "Against rule-based player:  init_win=82.00% follow_win=2.00% init_draw=0.00% follow_draw=0.00% total_win=42.00% total_lose=58.00%\n",
      "Step 13: trained on 2788 plays(d5). loss=0.0019433136330917478 \n",
      "Against rule-based player:  init_win=86.00% follow_win=0.00% init_draw=0.00% follow_draw=0.00% total_win=43.00% total_lose=57.00%\n",
      "Step 14: trained on 2413 plays(d2). loss=0.001242151134647429 \n",
      "Against rule-based player:  init_win=72.00% follow_win=0.00% init_draw=1.00% follow_draw=0.00% total_win=36.00% total_lose=63.50%\n",
      "Step 15: trained on 2435 plays(d3). loss=-0.01712431013584137 \n",
      "Against rule-based player:  init_win=84.00% follow_win=0.00% init_draw=1.00% follow_draw=0.00% total_win=42.00% total_lose=57.50%\n",
      "RL train finished! Policy update steps = 0\n"
     ]
    }
   ],
   "source": [
    "rl_trained_policy = reinforcement_learn(trained_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(rl_trained_policy.state_dict(), \"gomoku_rl_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 goes first: player1_wins=837 player2_wins=149 draw=14\n",
      "Player 2 goes first: player1_wins=14 player2_wins=983 draw=3\n",
      "Total: player1_wins=851 player2_wins=1132 draw=17\n",
      " init_win=83.70% follow_win=1.40% init_draw=1.40% follow_draw=0.30% total_win=42.55% total_lose=56.60%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4255"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head2head_eval(\n",
    "    PolicyPlayer(rl_trained_policy, greedy_play=False),\n",
    "    RuleBasedPlayer(),\n",
    "    num_plays=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 goes first: player1_wins=790 player2_wins=204 draw=6\n",
      "Player 2 goes first: player1_wins=427 player2_wins=565 draw=8\n",
      "Total: player1_wins=1217 player2_wins=769 draw=14\n",
      " init_win=79.00% follow_win=42.70% init_draw=0.60% follow_draw=0.80% total_win=60.85% total_lose=38.45%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6085"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head2head_eval(\n",
    "    PolicyPlayer(rl_trained_policy, greedy_play=False),\n",
    "    PolicyPlayer(trained_policy, greedy_play=False),\n",
    "    num_plays=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings from Reinforcement Learning\n",
    "1. For the first try, a reward is given for the win and a penalty is given for the loss. The draw games were dropped, which caused a problem. One explanation is that the policy quickly learned to avoid losses by playing defensively. During the following self-playing, the defensive policy played against another defensive policy, which caused a very high draw game ratio and no win/lose games for the policy to continue to learn.\n",
    "2. After adding a small penalty(-0.01) to both players for the draw games, the RL algorithm works pretty well. A model trained from one of the RL training runs has reached a 50% win rate against the rule based policy.\n",
    "3. RL training successfully fixed the issue of the first move. The model now knows to always put the first stone in the center.\n",
    "4. However, the RL training is not very stable. Only one of the many training runs achieved comparable performance as the rule-based player. Other runs have some improvement in the middle but not in the end. Only comparing with a single rule-based player is a biased evaluation, but that's the only available evaluation method for now.\n",
    "5. Tried smaller learning rate, cosine / exponential learning rate decay, smaller / larger batch sizes. Still not be able to steadily converging to the high performance model.\n",
    "6. Need to add some analysis if time allows, such as plotting out the activation and gradient distribution changes on each steps. After identifying the reasons of instability, things to try can be prioritized including adding norms, dropout, full connected layers, l2 regularizations, residuals, and etc. Another direction is to make the RL more efficient, by exploring MCTS, actor-critic or other more advanced RL algorithms.\n",
    "7. Given a model is already trained with quality on par with the rule based model, I think the model capacity is good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further findings with regards to the stability:\n",
    "1. Tried removed split reward by number of stones. Got another good model by luck. Stability is still an issue.\n",
    "2. Introduced a base model as opponent in self-play as the Alpha-Go setup. The policy plays with a fixed base model which is a snapshot of itself. The backprop only goes to the policy model that is on one side of each play as opposed to both sides in the case where the opponent is also the policy itself. In theory, this process is expected to be more stable since the model only get updated when it's better than itself. However, in practice the finding is that the target policy might be updated to a stage where it always becomes worse after each step, so that the base model never gets updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: trained on 959 plays(i9f9d0). loss=-0.407124400138855 \n",
      "Against rule-based player:  init_win=32.00% follow_win=0.00% init_draw=0.00% follow_draw=0.00% total_win=16.00% total_lose=84.00%\n",
      "Step 1: trained on 1068 plays(i10f11d2). loss=-0.18172159790992737 \n",
      "Against rule-based player:  init_win=23.00% follow_win=6.00% init_draw=0.00% follow_draw=0.00% total_win=14.50% total_lose=85.50%\n",
      "Step 2: trained on 958 plays(i13f7d1). loss=-0.0989319458603859 \n",
      "Against rule-based player:  init_win=28.00% follow_win=7.00% init_draw=0.00% follow_draw=0.00% total_win=17.50% total_lose=82.50%\n",
      "Step 3: trained on 926 plays(i16f17d0). loss=0.07061976939439774 \n",
      "Against rule-based player:  init_win=34.00% follow_win=8.00% init_draw=0.00% follow_draw=0.00% total_win=21.00% total_lose=79.00%\n",
      "Step 4: trained on 911 plays(i16f15d0). loss=-0.05327298864722252 \n",
      "Against rule-based player:  init_win=19.00% follow_win=11.00% init_draw=0.00% follow_draw=0.00% total_win=15.00% total_lose=85.00%\n",
      "Step 5: trained on 877 plays(i12f14d0). loss=-0.2289057970046997 \n",
      "Against rule-based player:  init_win=24.00% follow_win=9.00% init_draw=0.00% follow_draw=0.00% total_win=16.50% total_lose=83.50%\n",
      "Step 6: trained on 857 plays(i16f16d0). loss=-0.04721152037382126 \n",
      "Against rule-based player:  init_win=15.00% follow_win=9.00% init_draw=0.00% follow_draw=0.00% total_win=12.00% total_lose=88.00%\n",
      "Step 7: trained on 990 plays(i16f19d0). loss=0.02220037207007408 \n",
      "Against rule-based player:  init_win=21.00% follow_win=7.00% init_draw=0.00% follow_draw=0.00% total_win=14.00% total_lose=86.00%\n",
      "Step 8: trained on 966 plays(i16f17d1). loss=-0.009305174462497234 \n",
      "Against rule-based player:  init_win=24.00% follow_win=12.00% init_draw=0.00% follow_draw=0.00% total_win=18.00% total_lose=82.00%\n",
      "Step 9: trained on 927 plays(i20f15d0). loss=0.052813079208135605 \n",
      "Against rule-based player:  init_win=20.00% follow_win=11.00% init_draw=0.00% follow_draw=0.00% total_win=15.50% total_lose=84.50%\n",
      "Step 10: trained on 910 plays(i23f17d0). loss=0.06117507442831993 \n",
      "Against rule-based player:  init_win=22.00% follow_win=9.00% init_draw=0.00% follow_draw=0.00% total_win=15.50% total_lose=84.50%\n",
      "Step 11: trained on 979 plays(i18f15d0). loss=0.023244092240929604 \n",
      "Against rule-based player:  init_win=16.00% follow_win=10.00% init_draw=0.00% follow_draw=0.00% total_win=13.00% total_lose=87.00%\n",
      "Step 12: trained on 927 plays(i19f16d0). loss=-0.016180483624339104 \n",
      "Against rule-based player:  init_win=21.00% follow_win=12.00% init_draw=0.00% follow_draw=0.00% total_win=16.50% total_lose=83.50%\n",
      "Step 13: trained on 936 plays(i22f22d0). loss=0.20463339984416962 \n",
      "Against rule-based player:  init_win=25.00% follow_win=11.00% init_draw=0.00% follow_draw=0.00% total_win=18.00% total_lose=82.00%\n",
      "Step 14: trained on 938 plays(i22f20d0). loss=0.09841106086969376 \n",
      "Against rule-based player:  init_win=22.00% follow_win=11.00% init_draw=0.00% follow_draw=0.00% total_win=16.50% total_lose=83.50%\n",
      "Step 15: trained on 988 plays(i15f19d0). loss=-0.019648689776659012 \n",
      "Against rule-based player:  init_win=30.00% follow_win=7.00% init_draw=0.00% follow_draw=0.00% total_win=18.50% total_lose=81.50%\n",
      "RL train finished! Policy update steps = 0\n"
     ]
    }
   ],
   "source": [
    "rl_policy_with_fixed_base = reinforcement_learn(trained_policy, policy_as_player2=False, fixed_opponent_policy=rl_trained_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 goes first: player1_wins=709 player2_wins=289 draw=2\n",
      "Player 2 goes first: player1_wins=385 player2_wins=613 draw=2\n",
      "Total: player1_wins=1094 player2_wins=902 draw=4\n",
      " init_win=70.90% follow_win=38.50% init_draw=0.20% follow_draw=0.20% total_win=54.70% total_lose=45.10%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.547"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head2head_eval(\n",
    "    PolicyPlayer(rl_policy_with_fixed_base, greedy_play=False),\n",
    "    PolicyPlayer(rl_trained_policy, greedy_play=False),\n",
    "    num_plays=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intransitivity\n",
    "Tried to use the first successful RL policy (rl_trained_policy) as a fixed opponent to learn another policy(rl_policy_with_fixed_base). Head2head results showed some circular wins\n",
    "   - rule_based_policy wins rl_policy_with_fixed_base by 80.05%\n",
    "   - rl_policy_with_fixed_base wins rl_trained_policy by 57.6%\n",
    "   - rl_trained_policy ties rule_based_policy\n",
    "   This means that a policy can be trained to beat a specific opponent, but it's not guaranteed to be better than other possible policies. This is an intransitive game. This reveals 2 issues of the current LR setting:\n",
    "   1) Using one fixed rule based policy for the h2h evaluation is a pretty biased evaluation.\n",
    "   2) Using one single base policy in training is problematic. It's better to use a pool of models as the base policy. The Alpha-Go paper used a pool of models but it didn't mention why it stablizes the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: trained on 1006 plays(d0). loss=-0.911169171333313 value_loss=0.9993690848350525\n",
      "Against rule-based player:  init_win=26.00% follow_win=1.00% init_draw=1.00% follow_draw=0.00% total_win=13.50% total_lose=86.00%\n",
      "Step 1: trained on 1196 plays(d0). loss=-1.0305029153823853 value_loss=0.9995421171188354\n",
      "Against rule-based player:  init_win=26.00% follow_win=2.00% init_draw=3.00% follow_draw=4.00% total_win=14.00% total_lose=82.50%\n",
      "Step 2: trained on 1119 plays(d0). loss=-1.3956935405731201 value_loss=0.9994086027145386\n",
      "Against rule-based player:  init_win=12.00% follow_win=2.00% init_draw=8.00% follow_draw=3.00% total_win=7.00% total_lose=87.50%\n",
      "Step 3: trained on 1227 plays(d0). loss=-1.735711932182312 value_loss=0.9993572235107422\n",
      "Against rule-based player:  init_win=11.00% follow_win=2.00% init_draw=30.00% follow_draw=8.00% total_win=6.50% total_lose=74.50%\n",
      "Step 4: trained on 1516 plays(d0). loss=-1.1921594142913818 value_loss=0.9995474815368652\n",
      "Against rule-based player:  init_win=0.00% follow_win=9.00% init_draw=54.00% follow_draw=33.00% total_win=4.50% total_lose=52.00%\n",
      "Step 5: trained on 2158 plays(d6). loss=-1.0213937759399414 value_loss=0.6635048389434814\n",
      "Against rule-based player:  init_win=1.00% follow_win=4.00% init_draw=57.00% follow_draw=55.00% total_win=2.50% total_lose=41.50%\n",
      "Step 6: trained on 2159 plays(d9). loss=-1.2088311910629272 value_loss=0.4954184889793396\n",
      "Against rule-based player:  init_win=3.00% follow_win=4.00% init_draw=51.00% follow_draw=54.00% total_win=3.50% total_lose=44.00%\n",
      "Step 7: trained on 2486 plays(d11). loss=-1.2798662185668945 value_loss=0.4645862877368927\n",
      "Against rule-based player:  init_win=1.00% follow_win=2.00% init_draw=42.00% follow_draw=39.00% total_win=1.50% total_lose=58.00%\n",
      "Step 8: trained on 2707 plays(d12). loss=-1.3157947063446045 value_loss=0.4636174142360687\n",
      "Against rule-based player:  init_win=0.00% follow_win=1.00% init_draw=39.00% follow_draw=37.00% total_win=0.50% total_lose=61.50%\n",
      "Step 9: trained on 3209 plays(d18). loss=-1.0707461833953857 value_loss=0.3214578926563263\n",
      "Against rule-based player:  init_win=2.00% follow_win=1.00% init_draw=35.00% follow_draw=41.00% total_win=1.50% total_lose=60.50%\n",
      "Step 10: trained on 3671 plays(d20). loss=-0.8506316542625427 value_loss=0.34106209874153137\n",
      "Against rule-based player:  init_win=0.00% follow_win=0.00% init_draw=42.00% follow_draw=60.00% total_win=0.00% total_lose=49.00%\n",
      "Step 11: trained on 3569 plays(d18). loss=-1.1884145736694336 value_loss=0.39000535011291504\n",
      "Against rule-based player:  init_win=0.00% follow_win=0.00% init_draw=33.00% follow_draw=25.00% total_win=0.00% total_lose=71.00%\n",
      "Step 12: trained on 4024 plays(d21). loss=-1.0138670206069946 value_loss=0.36881932616233826\n",
      "Against rule-based player:  init_win=1.00% follow_win=0.00% init_draw=26.00% follow_draw=16.00% total_win=0.50% total_lose=78.50%\n",
      "Step 13: trained on 4257 plays(d24). loss=-1.0182267427444458 value_loss=0.31814005970954895\n",
      "Against rule-based player:  init_win=1.00% follow_win=0.00% init_draw=9.00% follow_draw=7.00% total_win=0.50% total_lose=91.50%\n",
      "Step 14: trained on 4067 plays(d20). loss=-1.1838757991790771 value_loss=0.4053012728691101\n",
      "Against rule-based player:  init_win=3.00% follow_win=0.00% init_draw=12.00% follow_draw=19.00% total_win=1.50% total_lose=83.00%\n",
      "Step 15: trained on 4482 plays(d26). loss=-1.5288816690444946 value_loss=0.29842787981033325\n",
      "Against rule-based player:  init_win=0.00% follow_win=0.00% init_draw=5.00% follow_draw=61.00% total_win=0.00% total_lose=67.00%\n",
      "RL train finished! Policy update steps = 0\n"
     ]
    }
   ],
   "source": [
    "mcts_trained_policy = reinforcement_learn(\n",
    "    trained_policy, use_mcts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: trained on 978 plays(d0). loss=-0.9526301622390747 value_loss=0.9992843270301819\n",
      "Against rule-based player:  init_win=21.00% follow_win=1.00% init_draw=2.00% follow_draw=1.00% total_win=11.00% total_lose=87.50%\n",
      "Step 1: trained on 1271 plays(d0). loss=-0.9089316129684448 value_loss=0.9996160268783569\n",
      "Against rule-based player:  init_win=23.00% follow_win=3.00% init_draw=9.00% follow_draw=1.00% total_win=13.00% total_lose=82.00%\n",
      "Step 2: trained on 1351 plays(d0). loss=-1.097440481185913 value_loss=0.9996745586395264\n",
      "Against rule-based player:  init_win=12.00% follow_win=3.00% init_draw=19.00% follow_draw=3.00% total_win=7.50% total_lose=81.50%\n",
      "Step 3: trained on 1585 plays(d0). loss=-1.0046199560165405 value_loss=0.9995349049568176\n",
      "Against rule-based player:  init_win=12.00% follow_win=2.00% init_draw=20.00% follow_draw=9.00% total_win=7.00% total_lose=78.50%\n",
      "Step 4: trained on 1570 plays(d1). loss=-1.312212347984314 value_loss=0.9224280714988708\n",
      "Against rule-based player:  init_win=7.00% follow_win=5.00% init_draw=38.00% follow_draw=22.00% total_win=6.00% total_lose=64.00%\n",
      "Step 5: trained on 1968 plays(d5). loss=-1.0925103425979614 value_loss=0.6923761367797852\n",
      "Against rule-based player:  init_win=5.00% follow_win=3.00% init_draw=50.00% follow_draw=33.00% total_win=4.00% total_lose=54.50%\n",
      "Step 6: trained on 1798 plays(d3). loss=-1.9269942045211792 value_loss=0.797785222530365\n",
      "Against rule-based player:  init_win=3.00% follow_win=2.00% init_draw=49.00% follow_draw=37.00% total_win=2.50% total_lose=54.50%\n",
      "Step 7: trained on 1935 plays(d4). loss=-2.236501455307007 value_loss=0.7496271729469299\n",
      "Against rule-based player:  init_win=3.00% follow_win=1.00% init_draw=34.00% follow_draw=37.00% total_win=2.00% total_lose=62.50%\n",
      "Step 8: trained on 2887 plays(d11). loss=-1.0879623889923096 value_loss=0.5390165448188782\n",
      "Against rule-based player:  init_win=1.00% follow_win=3.00% init_draw=27.00% follow_draw=30.00% total_win=2.00% total_lose=69.50%\n",
      "Step 9: trained on 2923 plays(d14). loss=-0.621558666229248 value_loss=0.42061832547187805\n",
      "Against rule-based player:  init_win=0.00% follow_win=0.00% init_draw=26.00% follow_draw=21.00% total_win=0.00% total_lose=76.50%\n",
      "Step 10: trained on 2474 plays(d8). loss=-1.4977788925170898 value_loss=0.608704149723053\n",
      "Against rule-based player:  init_win=2.00% follow_win=1.00% init_draw=21.00% follow_draw=12.00% total_win=1.50% total_lose=82.00%\n",
      "Step 11: trained on 2738 plays(d11). loss=-1.386458396911621 value_loss=0.5139014720916748\n",
      "Against rule-based player:  init_win=1.00% follow_win=0.00% init_draw=20.00% follow_draw=3.00% total_win=0.50% total_lose=88.00%\n",
      "Step 12: trained on 2680 plays(d12). loss=-2.4458723068237305 value_loss=0.45830702781677246\n",
      "Against rule-based player:  init_win=2.00% follow_win=0.00% init_draw=21.00% follow_draw=2.00% total_win=1.00% total_lose=87.50%\n",
      "Step 13: trained on 3017 plays(d11). loss=-2.080293655395508 value_loss=0.5590174198150635\n",
      "Against rule-based player:  init_win=3.00% follow_win=0.00% init_draw=26.00% follow_draw=4.00% total_win=1.50% total_lose=83.50%\n",
      "Step 14: trained on 2935 plays(d9). loss=-2.303684711456299 value_loss=0.6290427446365356\n",
      "Against rule-based player:  init_win=1.00% follow_win=0.00% init_draw=30.00% follow_draw=2.00% total_win=0.50% total_lose=83.50%\n",
      "Step 15: trained on 3456 plays(d15). loss=-2.323899269104004 value_loss=0.475100576877594\n",
      "Against rule-based player:  init_win=0.00% follow_win=0.00% init_draw=5.00% follow_draw=5.00% total_win=0.00% total_lose=95.00%\n",
      "RL train finished! Policy update steps = 0\n"
     ]
    }
   ],
   "source": [
    "mcts_trained_policies = reinforcement_learn(\n",
    "    trained_policy, use_mcts=True, return_snapshots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings with MCTS:\n",
    "1. Adding MCTS does improve the stability! However, every time the RL training converged to a very defensive policy soon, then further trained to lose to the rule-based policy.\n",
    "2. It's unclear why the model converges to defensive first. Diagonosing the cause is hard due to the intransitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5857cb865a4551a0aef35b1f7c3aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=600.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI plays at 4, 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player plays at 5, 5\n",
      "AI plays at 4, 5\n",
      "Player plays at 4, 6\n",
      "AI plays at 6, 4\n",
      "Player plays at 6, 5\n",
      "AI plays at 5, 4\n",
      "Player plays at 3, 4\n",
      "AI plays at 7, 4\n",
      "Player plays at 8, 4\n",
      "AI plays at 7, 5\n",
      "Player plays at 8, 6\n",
      "AI plays at 8, 5\n",
      "Player plays at 9, 6\n",
      "AI plays at 7, 6\n",
      "Player plays at 7, 3\n",
      "AI plays at 7, 7\n",
      "Player plays at 7, 8\n",
      "AI plays at 8, 7\n",
      "Player plays at 6, 7\n",
      "AI plays at 9, 10\n",
      "Player plays at 5, 6\n",
      "AI plays at 8, 9\n",
      "Player plays at 5, 7\n",
      "AI plays at 5, 9\n",
      "Player plays at 3, 5\n",
      "AI plays at 6, 8\n",
      "Player plays at 4, 7\n",
      "AI plays at 3, 7\n",
      "Player plays at 3, 8\n",
      "AI plays at 2, 9\n",
      "Player plays at 3, 6\n",
      "AI plays at 2, 6\n",
      "Player plays at 4, 8\n",
      "AI plays at 4, 9\n",
      "Player plays at 3, 9\n",
      "AI plays at 2, 10\n",
      "Player plays at 2, 8\n",
      "AI plays at 5, 8\n",
      "Player plays at 1, 7\n",
      "AI plays at 4, 10\n",
      "Player plays at 1, 6\n",
      "AI plays at 0, 6\n",
      "Player plays at 1, 8\n",
      "AI plays at 1, 9\n",
      "Player plays at 0, 8\n",
      "Player wins!\n"
     ]
    }
   ],
   "source": [
    "play = InteractivePlay(\n",
    "  ai_player=PolicyPlayer(mcts_trained_policies[6], greedy_play=False).play,\n",
    "  ai_first=True,\n",
    ")\n",
    "play.display_board()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "The RL training in this notbook improved the Gomoku policy performance siginficantly on top of the supervised learning. This notebook only explored the RL training within a small amount of computation. Only 1024 episodes (16 steps * batch size 64) are explored for each training run. The NN model size is also small, only ~500k parameters. At this scale, a lot of things already need to be right to make RL work.\n",
    "  - What I have done and turned out useful:\n",
    "    - One-hot embedding: improves model performance significantly\n",
    "    - MCTS: Critical for stability\n",
    "    - Small penalty for draw games: RL converges to local optimal quickly without this.\n",
    "  - What look critical but I have not implemented yet:\n",
    "    - A pool of base policies for training and evaluation.\n",
    "    - Adjusting rewards to avoid defensive policy.\n",
    "  - What I hope the policy can learn if more computation is available:\n",
    "    - The policy should learn some strategies that human often use, such as the \"double three\" and \"four three\" rules. In order to learn these strategies, multiple consecutive moves that construct a human stategy need to happen in the stochastic self-play multiple times. This requires many magnitudes of more iterations as well as a much larger model capacity to remember the patterns.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
